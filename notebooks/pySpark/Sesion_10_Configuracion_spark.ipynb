{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6956efa9-7ad7-4446-94c1-e2c207fcf290",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder\\\n",
    "    .config(\"spark.driver.memory\", \"5g\")\\\n",
    "    .appName(\"sesion_1\")\\\n",
    "    .master(\"local[*]\")\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f0baf5-eb72-4518-be33-2c89e7db6ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950670f6-dcc4-4437-9bca-c1bb365e606a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML\n",
    "display(HTML(\"<style>pre { white-space: pre !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc64f041-2e22-4457-8be7-80c31c3cf0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext.uiWebUrl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68dd8b15-900f-49ee-b046-dff518037c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as f\n",
    "import pyspark.sql.types as t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b206961-7a2a-433f-895e-26834d4cf524",
   "metadata": {},
   "outputs": [],
   "source": [
    "contracts_df = spark.read.parquet(\"../../resources/data/parquet/big_contracts\")\n",
    "\n",
    "clients_df = spark.read.parquet(\"../../resources/data/parquet/big_clients\")\n",
    "\n",
    "contracts_df.show(2)\n",
    "clients_df.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e70cc7-0eb9-47f4-a839-72c98e40f1f1",
   "metadata": {},
   "source": [
    "#### spark.sql.adaptive.enabled\n",
    "##### Default value: true since Apache Spark 3.2.0.\n",
    "Adaptive Query Execution (AQE) is an optimization technique in Spark SQL that makes use of the runtime statistics to choose the most efficient query execution plan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3824aecf-ced8-4b49-b200-0eddb1d01c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"false\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806cecb8-9aa7-4771-9e00-8258a3ca2734",
   "metadata": {},
   "outputs": [],
   "source": [
    "contracts_df\\\n",
    "    .groupBy(\"cod_titular\")\\\n",
    "    .agg(\n",
    "        f.max(\"fec_alta\"),\n",
    "        f.min(\"fec_alta\")\n",
    "        )\\\n",
    "    .count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0280c95f-a42b-450a-a8e2-13c73c95fd16",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5518eaf4-d8e2-4b92-b811-06dac0eebe8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "contracts_df\\\n",
    "    .groupBy(\"cod_titular\")\\\n",
    "    .agg(\n",
    "        f.max(\"fec_alta\"),\n",
    "        f.min(\"fec_alta\")\n",
    "        )\\\n",
    "    .count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91fa51d-17f3-45d6-b8b4-8fb1bdbc2e7b",
   "metadata": {},
   "source": [
    "#### spark.sql.autoBroadcastJoinThreshold\n",
    "##### Default value: 10485760 (10 MB)\n",
    "Configures the maximum size in bytes for a table that will be broadcast to all worker nodes when performing a join. By setting this value to -1, broadcasting can be disabled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8cc54a-8d69-487a-89af-eeb4a736d4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"false\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d850f4fc-d284-4703-a6f2-0213f32d20f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ddcab9-e3bc-4efb-aa99-304f8ed6427d",
   "metadata": {},
   "outputs": [],
   "source": [
    "contracts_df.join(clients_df, [\"id\"]).explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df267a9-73b0-4df9-8492-0b091d2afb46",
   "metadata": {},
   "outputs": [],
   "source": [
    "contracts_df.join(clients_df, [\"id\"]).write.mode(\"overwrite\").parquet(\"../../resources/data/parquet/t_broadcast_join\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4930a4ee-fb39-4e89-8209-43e7e982481a",
   "metadata": {},
   "outputs": [],
   "source": [
    "3*1024*1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e001c52-adeb-4b10-982a-6bb591450214",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", \"3145728\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2859c73-e92c-457b-8a75-6ad2a737aee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "contracts_df.join(clients_df, [\"id\"]).write.mode(\"overwrite\").parquet(\"../../resources/data/parquet/t_broadcast_join\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f5d447-cc7d-4ffa-8141-dbdc092076d8",
   "metadata": {},
   "source": [
    "#### spark.sql.adaptive.autoBroadcastJoinThreshold\n",
    "##### Default value: Same as spark.sql.autoBroadcastJoinThreshold\n",
    "Configures the maximum size in bytes for a table that will be broadcast to all worker nodes when performing a join. By setting this value to -1, broadcasting can be disabled. The default value is the same as spark.sql.autoBroadcastJoinThreshold. Note that, this config is used only in adaptive framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea8d812-2ebe-42cd-b5b8-7c4dfc6582a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee2137d-239e-4329-83c9-b597363c43bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.get(\"spark.sql.adaptive.autoBroadcastJoinThreshold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfee1ca1-c9d7-4a7c-9ac4-f3bf571eaec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "contracts_df.join(clients_df, [\"id\"]).write.mode(\"overwrite\").parquet(\"../../resources/data/parquet/t_broadcast_join\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1f9266-42b0-42ab-920a-d38149760fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.adaptive.autoBroadcastJoinThreshold\", \"2097152\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6dc91e4-f7ce-40ed-bd45-bd97055c4a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "contracts_df.join(clients_df, [\"id\"]).write.mode(\"overwrite\").parquet(\"../../resources/data/parquet/t_broadcast_join\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9b3b5e-5d99-4b57-b2d8-f19df3e9aaab",
   "metadata": {},
   "source": [
    "#### spark.sql.broadcastTimeout\n",
    "##### Default value: 300\n",
    "Timeout in seconds for the broadcast wait time in broadcast joins\n",
    "\n",
    "      \"java.util.concurrent.TimeoutException: Futures timed out after [300 seconds] at\"\n",
    "\n",
    "This error is fixed by increasing the value of spark.sql.broadcastTimeout, for example e spark.conf.set(\"spark.sql.broadcastTimeout\", \"3600\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4ded77-b480-4f97-8690-99c20f724eb3",
   "metadata": {},
   "source": [
    "#### spark.default.parallelism\n",
    "##### Default number of partitions in **RDDs** returned by transformations like join, reduceByKey, and parallelize when not set by user.\n",
    "For distributed shuffle operations like reduceByKey and join, the largest number of partitions in a parent RDD. For operations like parallelize with no parent RDDs, it depends on the cluster manager:\n",
    "- Local mode: number of cores on the local machine\n",
    "- Mesos fine grained mode: 8\n",
    "- Others: total number of cores on all executor nodes or 2, whichever is larger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3e215c-5f3f-4862-8315-41663bf549bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.sparkContext.textFile(\"../../resources/data/txt/green_eggs_and_ham.txt\")\n",
    "data.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4212f10a-64a0-4b1d-8274-b5ffe8045664",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = data \\\n",
    "    .flatMap(lambda line: line.replace(\"  \", \" \").upper().split(\" \")) \\\n",
    "    .map(lambda word: (word, 1)) \\\n",
    "    .reduceByKey(lambda count_1, count_2: count_1 + count_2)\n",
    "result.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd6a1a2-3f08-4d45-a91a-09d1dd678c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53321689-c93d-42af-8f8c-2cf1c055918a",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.default.parallelism\", 17) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ec300d-99ff-4233-a6ea-0fe09fb9f52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = data \\\n",
    "    .flatMap(lambda line: line.replace(\"  \", \" \").upper().split(\" \")) \\\n",
    "    .map(lambda word: (word, 1)) \\\n",
    "    .reduceByKey(lambda count_1, count_2: count_1 + count_2)\n",
    "result.count()   #No funciona spark.default.parallelism una vez creada la spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d35a340-6fe2-4163-838c-1bc339d94736",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc81fab1-855b-4cbc-b545-8ea0cebc4495",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder\\\n",
    "    .config(\"spark.driver.memory\", \"5g\")\\\n",
    "    .config(\"spark.default.parallelism\", \"17\")\\\n",
    "    .appName(\"sesion_1\")\\\n",
    "    .master(\"local[*]\")\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd0a8a0-9a09-4d67-a857-6a206076c06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.sparkContext.textFile(\"../../resources/data/txt/green_eggs_and_ham.txt\", 8)\n",
    "data.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b811f5dc-e755-4677-b5c4-f33f006fa63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = data \\\n",
    "    .flatMap(lambda line: line.replace(\"  \", \" \").upper().split(\" \")) \\\n",
    "    .map(lambda word: (word, 1)) \\\n",
    "    .reduceByKey(lambda count_1, count_2: count_1 + count_2)\n",
    "result.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41baf37-dd3a-494d-ae9a-df70b22ee01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7abb67-959d-45ee-8976-297386934a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.sparkContext.parallelize([i for i in range(100)])\n",
    "data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e15b43-bc57-4bf5-a7ef-21d4f328ef6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bddf9c8-5c8f-4799-ae7e-61a85a48d4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.get(\"spark.default.parallelism\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b02b3e-52a5-4731-b24d-9944a00f74b6",
   "metadata": {},
   "source": [
    "#### spark.sql.shuffle.partitions\n",
    "##### Default value: 200\n",
    "The default number of partitions to use when shuffling data for joins or aggregations. Used in DataFrames or DataSets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113b9bd4-e3f5-4eff-8f3b-0991f34feb27",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"false\")\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdcfc269-fdc7-41de-ae8d-cc933c48e19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "contracts_df.join(clients_df, [\"id\"]).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577b02bb-031b-4e58-81d5-51f98b636dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\",\"100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea33b5d-8669-4ea5-baae-6d2434cf218f",
   "metadata": {},
   "outputs": [],
   "source": [
    "contracts_df.join(clients_df, [\"id\"]).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73325f02-f400-48d6-bd0d-21d69c35a5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.get(\"spark.sql.shuffle.partitions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09fff348-6c39-4305-892b-4033cf3cea6f",
   "metadata": {},
   "source": [
    "#### spark.sql.files.maxPartitionBytes\t\n",
    "##### Default value: 134217728 (128 MB)\n",
    "The maximum number of bytes to pack into a single partition when reading files. This configuration is effective only when using file-based sources such as Parquet, JSON and ORC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2c97c1-0f88-40d9-a2b7-20f2cf364ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "contracts_df.join(clients_df, [\"id\"]).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbdd5227-bfa5-4a7f-b63b-310a9e12b190",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.get(\"spark.sql.files.maxPartitionBytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2c078d-39cc-43e5-b63c-2b13daa47203",
   "metadata": {},
   "outputs": [],
   "source": [
    "128*1024*1024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ef749c-4257-4c7f-8a96-e9c0d16b063a",
   "metadata": {},
   "source": [
    "#### spark.sql.adaptive.skewJoin.enabled\n",
    "##### Default value: true\n",
    "When true and spark.sql.adaptive.enabled is true, Spark dynamically handles skew in sort-merge join by splitting (and replicating if needed) skewed partitions.\n",
    "\n",
    "https://spark.apache.org/docs/latest/sql-performance-tuning.html#optimizing-skew-join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018631fe-1ad9-4ff4-b71a-6621dff27050",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\")\n",
    "spark.conf.set(\"spark.sql.adaptive.autoBroadcastJoinThreshold\", \"-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30a6859-ebbe-4d51-b5cb-42f4f9bf1445",
   "metadata": {},
   "outputs": [],
   "source": [
    "contracts_df.groupBy(\"cod_titular\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0968d27d-1913-49ce-b854-815b76959f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "clients_df.sample(False, 0.00003, 0).groupBy(\"cod_client\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b3322b-f4e2-4ce4-a52e-45ffe168f3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "contracts_df.join(clients_df.sample(False, 0.00003, 0).drop(\"id\"), f.col(\"cod_titular\") == f.col(\"cod_client\")).explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e77503-0063-47c4-8d43-81236383cce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "contracts_df.join(clients_df.sample(False, 0.00003, 0).drop(\"id\"), f.col(\"cod_titular\") == f.col(\"cod_client\"))\\\n",
    "    .write.mode(\"overwrite\").parquet(\"../../resources/data/parquet/t_skew_join\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac0df23-9fc7-4aed-8939-b76af1ca5c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.adaptive.skewJoin.enabled\",\"true\")\n",
    "spark.conf.set(\"spark.sql.adaptive.skewJoin.skewedPartitionFactor\", \"2.0\")\n",
    "spark.conf.set(\"spark.sql.adaptive.skewJoin.skewedPartitionThresholdInBytes\", \"50m\")\n",
    "spark.conf.set(\"spark.sql.adaptive.advisoryPartitionSizeInBytes\", \"10m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fdb1f88-90c1-460c-8182-a0dcd7bf9b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "contracts_df.join(clients_df.sample(False, 0.00003, 0).drop(\"id\"), f.col(\"cod_titular\") == f.col(\"cod_client\"))\\\n",
    "    .write.mode(\"overwrite\").parquet(\"../../resources/data/parquet/t_skew_join\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9927a0d1-a20e-45a7-bb24-686a8072629c",
   "metadata": {},
   "source": [
    "#### spark.sql.adaptive.coalescePartitions.enabled\n",
    "##### Default value: true\n",
    "When true and spark.sql.adaptive.enabled is true, Spark will coalesce contiguous shuffle partitions according to the target size (specified by spark.sql.adaptive.advisoryPartitionSizeInBytes), to avoid too many small tasks.\n",
    "\n",
    "https://spark.apache.org/docs/latest/sql-performance-tuning.html#coalescing-post-shuffle-partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1722fa4b-6046-4fd3-a952-ded322e6b308",
   "metadata": {},
   "outputs": [],
   "source": [
    "contracts_df = spark.read.parquet(\"../../resources/data/parquet/big_contracts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48df734f-b205-453a-86d5-1b66934c73bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"false\")\n",
    "\n",
    "contracts_df.repartition(\"id\")\\\n",
    "    .write.mode(\"overwrite\").parquet(\"../../resources/data/parquet/t_skew_join\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479baa15-9093-485d-b9fd-2c0e8d91b583",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.get(\"spark.sql.adaptive.coalescePartitions.enabled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebaa377b-d74e-44a9-a26c-92c8f22b39cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "contracts_df.repartition(\"id\")\\\n",
    "    .write.mode(\"overwrite\").parquet(\"../../resources/data/parquet/t_skew_join\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f88f1d4-b305-4505-9547-ec3f129c7672",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.adaptive.coalescePartitions.parallelismFirst\", \"false\")\n",
    "spark.conf.set(\"spark.sql.adaptive.advisoryPartitionSizeInBytes\", \"64m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36b977d-8e8d-49e5-968f-7c6041e0e54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "contracts_df.repartition(\"id\")\\\n",
    "    .write.mode(\"overwrite\").parquet(\"../../resources/data/parquet/t_skew_join\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea621c79-dd00-4b0a-abf9-72652e7f5e38",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
