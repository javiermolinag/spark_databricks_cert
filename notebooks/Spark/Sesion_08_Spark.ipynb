{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d281bf6f-8325-4b44-9dd1-e69bab90af2c",
   "metadata": {},
   "source": [
    "### Just Enough Spark! Core Concepts Revisited !!\n",
    "\n",
    "**Apache Spark™** is a unified analytics engine for large-scale data processing. Its a lightning-fast engine for big data and machine learning. The largest open source project in data processing. It works seamlessly on almost all open source big data technologies.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7549ce1-1f90-4016-a344-d2d7d4dc11e8",
   "metadata": {},
   "source": [
    "### Spark Basic Architecture\n",
    "\n",
    "A cluster, or group of machines, pools the resources of many machines together allowing us to use all the cumulative resources as if they were one. Now a group of machines sitting somewhere alone is not powerful, you need a framework to coordinate work across them. Spark is a tailor-made engine exactly for this, managing and coordinating the execution of tasks on data across a cluster of computers. \n",
    "\n",
    "The cluster of machines that Spark will leverage to execute tasks will be managed by a cluster manager like Spark’s Standalone cluster manager, YARN - Yet Another Resource Negotiator, Kubernetes. We then submit Spark Applications to these cluster managers which will grant resources to our application so that we can complete our work. \n",
    "```\n",
    "                                                            +-----------------------------------------+\n",
    "                                                            |  Worker Node                            |\n",
    "                                                            |  +----------------+  +----------------+ |\n",
    "                                                            |  |    Executor    |  |    Executor    | |\n",
    "                             +----------------------------> |  | +----+  +----+ |  | +----+  +----+ | |\n",
    "                             |                              |  | |Task|  |Task| |  | |Task|  |Task| | |\n",
    "                             |                          +-> |  | +----+  +----+ |  | +----+  +----+ | |\n",
    "                             |                          |   |  +----------------+  +----------------+ |\n",
    "       +----------------+    |                          |   +-----------------------------------------+\n",
    "       | Driver Program | <--+    +-----------------+   |\n",
    "       |                |         |                 | <-+\n",
    "       | +------------+ | <-----> | Cluster Manager |\n",
    "       | |SparkContext| |         |                 | <-+\n",
    "       | +------------+ | <--+    +-----------------+   |   +-----------------------------------------+\n",
    "       +----------------+    |                          |   |  Worker Node                            |\n",
    "                             |                          |   |  +----------------+ +----------------+  |\n",
    "                             |                          +-> |  |    Executor    | |    Executor    |  |\n",
    "                             |                              |  | +----+  +----+ | | +----+  +----+ |  |\n",
    "                             +----------------------------> |  | |Task|  |Task| | | |Task|  |Task| |  |\n",
    "                                                            |  | +----+  +----+ | | +----+  +----+ |  |\n",
    "                                                            |  +----------------+ +----------------+  |\n",
    "                                                            +-----------------------------------------+\n",
    "```\n",
    "\n",
    "Spark Applications consist of a driver process and a set of executor processes. In the illustration we see above, our driver is on the left and two executors on the right."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34f7ce8-ae49-4922-905c-fd4e4ed87a08",
   "metadata": {},
   "source": [
    "### Deployment modes\n",
    "\n",
    "1. **Cluser mode:**\n",
    "    * The Spark Driver is launched on a worker node.\n",
    "    * The cluster manager is responsible for Spark processes.\n",
    "```\n",
    "               +-----------------+\n",
    "               | Cluster Manager |\n",
    "               +--------+--------+\n",
    "                        |\n",
    "       +----------------+----------------+\n",
    "      _|_              _|_              _|_\n",
    "      \\_/              \\_/              \\_/\n",
    "+-------------+  +-------------+  +-------------+\n",
    "| Worker node |  | Worker node |  | Worker node |\n",
    "| +---+ +---+ |  | +---+ +---+ |  | +---+ +---+ |\n",
    "| | D | | E | |  | | E | | E | |  | | E | | E | |\n",
    "| +---+ +---+ |  | +---+ +---+ |  | +---+ +---+ |\n",
    "+-------------+  +-------------+  +-------------+\n",
    "```\n",
    "2.  **Client mode:**\n",
    "    * The Spark Driver is on the client machine (runs on a machine that is not part of the cluster). That client is responsible for mantaining the Spark driver.\n",
    "    * The cluster manager is responsible for Spark processes.\n",
    "```\n",
    "               +-----------------+              +----------------+\n",
    "               | Cluster Manager |<-------------| Client machine |\n",
    "               +--------+--------+              | +--------+     |\n",
    "                        |                       | | Driver |     |\n",
    "       +----------------+----------------+      | +--------+     |\n",
    "      _|_              _|_              _|_     +----------------+\n",
    "      \\_/              \\_/              \\_/\n",
    "+-------------+  +-------------+  +-------------+\n",
    "| Worker node |  | Worker node |  | Worker node |\n",
    "| +---+ +---+ |  | +---+ +---+ |  | +---+ +---+ |\n",
    "| | E | | E | |  | | E | | E | |  | | E | | E | |\n",
    "| +---+ +---+ |  | +---+ +---+ |  | +---+ +---+ |\n",
    "+-------------+  +-------------+  +-------------+\n",
    "```\n",
    "3.  **Local mode:**\n",
    "    * Spark Driver runs on a single machine and the driver and executors run as separate processes inside the same machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d304edf-3c0c-41b8-86e5-6d2a4ef8cc86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "\n",
    "val spark = SparkSession.builder\n",
    "    .appName(\"Spark app\")\n",
    "    .master(\"local[*]\")\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38996950-c751-45c2-9cba-008b3309089c",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext.uiWebUrl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d95cdc-78ad-4ff6-a0cc-1546cc73b78d",
   "metadata": {},
   "source": [
    "### Cluster Manager\n",
    "1. One node manages the state of the cluster\n",
    "2. The other nodes do the work\n",
    "3. Communicate via driver/worker processes\n",
    "\n",
    "Examples\n",
    "* Standalone – a simple cluster manager included with Spark that makes it easy to set up a cluster.\n",
    "* Apache Mesos – a general cluster manager that can also run Hadoop MapReduce and service applications. (Deprecated)\n",
    "* Hadoop YARN – the resource manager in Hadoop 2 and 3.\n",
    "* Kubernetes – an open-source system for automating deployment, scaling, and management of containerized applications. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f362657c-f4fe-45f5-b53c-b5648f6f928f",
   "metadata": {},
   "source": [
    "### What is a Worker node?\n",
    "Any node that can run application code in the cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e37466a-15f3-41b1-80b2-ca4762ce59d6",
   "metadata": {},
   "source": [
    "### What is a Driver?\n",
    "\n",
    "The driver process runs your main() function, sits on the node in the cluster and is responsible for 3 main things:\n",
    "\n",
    "1. Maintaining information about the spark application.\n",
    "2. Responding to user’s program or input.\n",
    "3. Analyzing, distributing and scheduling work across the executors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc48bc0-6e61-483e-839a-e32028b1037b",
   "metadata": {},
   "source": [
    "### What is a Executor?\n",
    "\n",
    "The executors are responsible for carrying out the work that the driver assigns them. \n",
    "\n",
    "Executors are launched in JVM containers with their own memory/CPU resources.\n",
    "\n",
    "1. Execute code assigned to it by the driver.\n",
    "2. Reporting the state of the computation on that executor back to driver."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7cc5eb-426b-427f-bceb-c0f05eabd250",
   "metadata": {},
   "source": [
    "### What is a JVM?\n",
    "\n",
    "The JVM manages system memory and provides a portable execution environment for Java-based applications.\n",
    "\n",
    "**Technical definition:** The JVM is the specification for a software program that executes code and provides the runtime environment for that code. \n",
    "\n",
    "**Everyday definition:** The JVM is how we run our Java programs. We configure the JVM's settings and then rely on it to manage program resources during execution. \n",
    "\n",
    "The Java Virtual Machine (JVM) is a program whose purpose is to execute other programs. \n",
    "\n",
    "The JVM has two primary functions: \n",
    "\n",
    "1. To allow Java programs to run on any device or operating system (known as the \"Write once, run anywhere\" principle)\n",
    "2. To manage and optimize program memory "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3114bf8e-20c9-41f5-8add-06600316b566",
   "metadata": {},
   "source": [
    "### What is Cores/Slots/Threads?\n",
    "\n",
    "Spark parallelizes at two levels. One is the splitting the work among executors. The other is the slot. Each executor has a number of slots. Each slot can be assigned a Task. \n",
    "\n",
    "For example: the diagram below is showing 2 Core Executor nodes: \n",
    "```\n",
    "                                         +--------+\n",
    "                                         | Driver |\n",
    "                                         +--------+JVM\n",
    "                                              |\n",
    "         +------------------------+-----------+------------+------------------------+\n",
    "         |                        |                        |                        |\n",
    "         |                        |                        |                        |\n",
    "+----------------+       +----------------+       +----------------+       +----------------+\n",
    "|    Executor    |       |    Executor    |       |    Executor    |       |    Executor    |\n",
    "| +----+  +----+ |       | +----+  +----+ |       | +----+  +----+ |       | +----+  +----+ |\n",
    "| |Task|  |Task| |       | |Task|  |Slot| |       | |Slot|  |Slot| |       | |Task|  |Task| |\n",
    "| +----+  +----+ |       | +----+  +----+ |       | +----+  +----+ |       | +----+  +----+ |\n",
    "+----------------+JVM    +----------------+JVM    +----------------+JVM    +----------------+JVM\n",
    "```\n",
    "\n",
    "* The JVM is naturally multithreaded, but a single JVM, such as our Driver, has a finite upper limit.\n",
    "* By creating Tasks, the Driver can assign units of work to Slots on each Executor for parallel execution.\n",
    "* Additionally, the Driver must also decide how to partition the data so that it can be distributed for parallel processing (see below).\n",
    "* Consequently, the Driver is assigning a Partition of data to each task - in this way each Task knows which piece of data it is to process.\n",
    "* Once started, each Task will fetch from the original data source (e.g. An Azure Storage Account) the Partition of data assigned to it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212fa39a-def0-416f-8c01-33141f664539",
   "metadata": {},
   "source": [
    "### What is a Partition?\n",
    "\n",
    "In order to allow every executor to perform work in parallel, Spark breaks up the data into chunks, called partitions.\n",
    "```\n",
    "+-------------------+           +-------------------+\n",
    "|                   |   --->    |    PARTITION 1    |\n",
    "|                   |           +-------------------+\n",
    "|                   |   --->    |    PARTITION 2    |\n",
    "|                   |           +-------------------+\n",
    "|       BIG         |   --->    |    PARTITION 3    |\n",
    "|      TABLE        |           +-------------------+\n",
    "|                   |   --->    |    PARTITION 4    |\n",
    "|                   |           +-------------------+\n",
    "|                   |   --->    |    PARTITION 5    |\n",
    "|                   |           +-------------------+\n",
    "|                   |   --->    |        ...        |\n",
    "|                   |           +-------------------+\n",
    "|                   |   --->    |    PARTITION N    |\n",
    "+-------------------+           +-------------------+\n",
    "```\n",
    "A partition is a collection of rows that sit on one physical machine in our cluster. A DataFrame’s partitions represent how the data is physically distributed across your cluster of machines during execution:\n",
    "\n",
    "* If you have one partition, Spark will only have a parallelism of one, even if you have thousands of executors. \n",
    "* If you have many partitions, but only one executor, Spark will still only have a parallelism of one because there is only one computation resource. \n",
    "\n",
    "An important thing to note is that with DataFrames, we do not (for the most part) manipulate partitions manually (on an individual basis). We simply specify high level transformations of data in the physical partitions and Spark determines how this work will actually execute on the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5d12c2-524c-4e02-aba7-17205776bc73",
   "metadata": {},
   "outputs": [],
   "source": [
    "val clients_df = spark.read.parquet(\"../../resources/data/parquet/big_clients\")\n",
    "val contracts_df = spark.read.parquet(\"../../resources/data/parquet/big_contracts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8337aa6c-6477-41d5-bb08-da005a8a82ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.{functions => f}\n",
    "clients_df.groupBy(f.spark_partition_id()).count().show()\n",
    "contracts_df.groupBy(f.spark_partition_id()).count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e8848a-1f5a-4b95-90de-875ed5206929",
   "metadata": {},
   "source": [
    "### What is a DAG?\n",
    "Directed Acyclic Graph ( DAG ) in Apache Spark is a set of Vertices and Edges, where vertices represent the RDDs and the edges represent the Operation to be applied on RDDs.\n",
    "\n",
    "DAGScheduler is the scheduling layer of Apache Spark that implements stage-oriented scheduling. It transforms a logical execution plan to a physical execution plan (using stages).\n",
    "\n",
    "After an action has been called, SparkContext hands over a logical plan to DAGScheduler that it in turn translates to a set of stages that are submitted as a set of tasks for execution.\n",
    "\n",
    "The fundamental concepts of DAGScheduler are jobs and stages that it tracks through internal registries and counters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6284f761-90aa-4c26-8249-31f1c6c932f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "contracts_df.join(clients_df, Seq(\"id\"), \"outer\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252c7084-46f4-4a31-8435-8793b1e4a69d",
   "metadata": {},
   "source": [
    "### Transformations Vs Actions\n",
    "\n",
    "#### Transformations\n",
    "\n",
    "In Spark, the core data structures are immutable meaning they cannot be changed once created. In order to \"change\" a DataFrame you will have to instruct Spark how you would like to modify the DataFrame you have into the one that you want. These instructions are called transformations. Examples – **Select, Filter, GroupBy, Join, Union, Repartition, etc**\n",
    "\n",
    "#### Actions\n",
    "\n",
    "Transformations allow us to build up our logical transformation plan. To trigger the computation, we run an action. An action instructs Spark to compute a result from a series of transformations. Examples – **count, write, show and collect**.\r\n",
    "### Narrow Transformations Vs Wide Transformations\n",
    "\n",
    "There are two types of transformations: Narrow and Wide.\n",
    "\n",
    "For **narrow transformations**, the data required to compute the records in a single partition reside in at most one partition of the parent dataset. \n",
    "\n",
    "Examples include:\n",
    "\n",
    "* filter(..)\n",
    "* drop(..)\n",
    "* coalesce()\n",
    "\n",
    "For **wide transformations**, the data required to compute the records in a single partition may reside in many partitions of the parent dataset. \n",
    "\n",
    "Examples include:\n",
    "\n",
    "* distinct()\n",
    "* groupBy(..).agg()\n",
    "* repartition(n)\n",
    "* join()\n",
    "\n",
    "Remember, spark partitions are collections of rows that sit on physical machines in the cluster. Narrow transformations mean that work can be computed and reported back to the executor without changing the way data is partitioned over the system. Wide transformations require that data be redistributed over the system. This is called a shuffle. \n",
    "\n",
    "Shuffles are triggered when data needs to move between executors. \r\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce1739f-6416-4255-a59e-592eeee8a417",
   "metadata": {},
   "outputs": [],
   "source": [
    "contracts_df.join(clients_df, [\"id\"], \"outer\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933a6a03-ac80-48ce-a94f-63d31a45eb25",
   "metadata": {},
   "source": [
    "### What is a Shuffling?\n",
    "\n",
    "A Shuffle refers to an operation where data is re-partitioned across a Cluster - i.e. when data needs to move between executors.\n",
    "\n",
    "join and any operation that ends with ByKey will trigger a Shuffle. It is a costly operation because a lot of data can be sent via the network.\n",
    "\n",
    "For example, to group by letters, it will serve us best if...\n",
    "\n",
    "All the A's are in one partitions\n",
    "All the B's are in a second partition\n",
    "All the C'S are in a third\n",
    "From there we can easily sum/count/average all of the A's, B's, and C's.\n",
    "\n",
    "```\n",
    "+---------------------------+                 +---------------------------+\n",
    "|          STAGE 1          |                 |          STAGE 2          |\n",
    "| +-------+       +-------+ |                 | +-------+                 |\n",
    "| |       | ----> |AAAAAAA| |                 | |AAAAAAA|       +-------+ |\n",
    "| |       | ----> |AAAAAAA| |                 | |AAAAAAA| ----> |   4   | |\n",
    "| |       | ----> |BBBBBBB| |                 | |AAAAAAA|       +-------+ |\n",
    "| |       | ----> |CCCCCCC| |                 | |AAAAAAA|                 |\n",
    "| +-------+       +-------+ |                 | +-------+                 |\n",
    "| +-------+       +-------+ |                 | +-------+                 |\n",
    "| |       | ----> |BBBBBBB| |         >       | |BBBBBBB|       +-------+ |\n",
    "| |       | ----> |CCCCCCC| |   ------>>>     | |BBBBBBB| ----> |   4   | |\n",
    "| |       | ----> |BBBBBBB| |   ------>>>>>   | |BBBBBBB|       +-------+ |\n",
    "| |       | ----> |AAAAAAA| |   ------>>>     | |BBBBBBB|                 |\n",
    "| +-------+       +-------+ |         >       | +-------+                 |\n",
    "| +-------+       +-------+ |                 | +-------+                 |\n",
    "| |       | ----> |BBBBBBB| |                 | |CCCCCCC|       +-------+ |\n",
    "| |       | ----> |AAAAAAA| |                 | |CCCCCCC| ----> |   4   | |\n",
    "| |       | ----> |CCCCCCC| |                 | |CCCCCCC|       +-------+ |\n",
    "| |       | ----> |CCCCCCC| |                 | |CCCCCCC|                 |\n",
    "| +-------+       +-------+ |                 | +-------+                 |\n",
    "+---------------------------+                 +---------------------------+\n",
    "             MAP                  SHUFFLE                REDUCE\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e6de71-b2f5-47a9-a553-df44d5910659",
   "metadata": {},
   "source": [
    "### What is a Job?\n",
    "\n",
    "A **Job** is a sequence of stages, triggered by an action such as count(), collect(), read() or write(). \n",
    "\n",
    "* Each parallelized action is referred to as a Job.\n",
    "* The results of each Job (parallelized/distributed action) is returned to the Driver from the Executor.\n",
    "* Depending on the work required, multiple Jobs will be required."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf06d1e-96e0-491b-bee2-160fed57c306",
   "metadata": {},
   "source": [
    "### What is a Stage?\n",
    "Each job that gets divided into smaller sets of tasks is a **stage**.\n",
    "\n",
    "A **Stage** is a sequence of Tasks that can all be run together - i.e. in parallel - without a shuffle. For example: using \"**.read**\" to read a file from disk, then runnning \"**.filter**\" can be done without a shuffle, so it can fit in a single stage. The number of Tasks in a Stage also depends upon the number of Partitions your datasets have.\n",
    "```\n",
    "                                           +-------+\n",
    "                                           | Stage |---+\n",
    "                                           +-------+ge |---+\n",
    "+--------+                  +-----+            +-------+ge |---+\n",
    "| Action | ---> Submit ---> | Job | ------------>  +-------+ge |---+\n",
    "+--------+                  +-----+                    +-------+ge |\n",
    "                                                           +-------+\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a0cf38-10a8-4954-98bd-7b3da7276445",
   "metadata": {},
   "source": [
    "### What is a Task?\n",
    "\n",
    "A task is the smallest unit of work that is sent to the executor. Each stage has some tasks, one task per partition. The same task is done over different partitions of the RDD.\n",
    "\n",
    "```\n",
    "+-------------------+                                               +-------------------+\n",
    "|        RDD        |                                               |       Stage       |-+\n",
    "| +---------------+ |                                               | +---------------+ | |-+\n",
    "| |  PARTITION 1  | |                                               | |     Task      | | | |\n",
    "| +---------------+ |                                               | +---------------+ | | |\n",
    "| +---------------+ |                                               | +---------------+ | | |\n",
    "| |  PARTITION 2  | |                                               | |     Task      | | | |\n",
    "| +---------------+ |                                               | +---------------+ | | |\n",
    "| +---------------+ |      +--------+                  +-----+      | +---------------+ | | |\n",
    "| |  PARTITION 3  | | ---> | Action | ---> Submit ---> | Job | ---> | |     Task      | | | |\n",
    "| +---------------+ |      +--------+                  +-----+      | +---------------+ | | |\n",
    "| +---------------+ |                                               | +---------------+ | | |\n",
    "| |  PARTITION 4  | |                                               | |     Task      | | | |\n",
    "| +---------------+ |                                               | +---------------+ | | |\n",
    "|        ...        |                                               |        ...        | | |\n",
    "| +---------------+ |                                               | +---------------+ | | |\n",
    "| |  PARTITION 15  | |                                               | |     Task      | | | |\n",
    "| +---------------+ |                                               | +---------------+ | | |\n",
    "+-------------------+                                               +-------------------+ | |\n",
    "                                                                      +-------------------+ |\n",
    "                                                                        +-------------------+\n",
    "```\n",
    "\n",
    "In the example of **Stages** above, each **Step** is a **Task**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4344af4b-6da1-46fd-9f03-55010c7d60eb",
   "metadata": {},
   "source": [
    "### Spark app execution\n",
    "\n",
    "* An action triggers a Job\n",
    "\n",
    "* A job is split into stages\n",
    "    * each stage is dependent on the stage before it\n",
    "    * a stage must fully complete before the next stage can start\n",
    "    * for performance (usually) minimize the number of stages\n",
    "\n",
    "* A stage has tasks\n",
    "    * task = smallest unit of work\n",
    "    * tasks are run by executors\n",
    "\n",
    "* An RDD/DF/DS has partitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a00cb3-1782-42ad-851b-40fb932fadff",
   "metadata": {},
   "source": [
    "### Concepts Relationship\n",
    "App decomposition\n",
    "* 1 job = 1 or more stages\n",
    "* 1 stage = 1 or more tasks\n",
    "\n",
    "Tasks & executors\n",
    "* 1 task is run by 1 executor\n",
    "* each executor can run 0 or more tasks\n",
    "\n",
    "Partitions and tasks\n",
    "* processing 1 partition = one task\n",
    "\n",
    "Partitions & executors\n",
    "* 1 partition stays on 1 executor\n",
    "* each executor can load 0 or more partitions in memory or disk\n",
    "\n",
    "Executors & nodes\n",
    "* 1 executor = 1 JVM on 1 physical node\n",
    "* each physical node can have 0 or more executors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85fa1377-6174-42d9-b951-977add17837a",
   "metadata": {},
   "source": [
    "### Catalyst Optimizer\n",
    "\n",
    "1. Catalyst Optimizer and Tungsten Execution Engine was introduced in Spark 1.x\n",
    "2. Cost-Based Optimizer was introduced in Spark 2.x\n",
    "3. Adaptive Query Execution now got introduced in Spark 3.x\n",
    "\n",
    "To disable the Adaptive Query Execution -> spark.conf.set(\"spark.sql.adaptive.enabled\", False)\n",
    "\n",
    "Only works with DF and DS\n",
    "\n",
    "![title](../../resources/img/Catalist_Optimizer.PNG)\n",
    "\n",
    "* Parser: The query is parsed to create an abstract syntax tree (AST), representing the logical structure of the query.\n",
    "* Analyzer: It performs semantic analysis on the AST. It resolves table and column names, checks for syntax errors, and applies type checking to ensure the query is valid.\n",
    "* Optimizer: It applies a set of logical optimizations -Simplify boolan expressions, push filters to data sources, etc.\n",
    "* Planner: It generates various alternative physical plans based on the logical plan. It explores different execution strategies. It leverages statistics and cost models to estimate the cost of each physical plan. It considers factors like data distribution, network latency, disk I/O, CPU usage, and memory consumption. Using these cost estimates, it selects the physical plan with the lowest estimated cost.\n",
    "* Query execution: Once the physical plan is selected, the Catalyst Optimizer generates efficient Java bytecode or optimized Spark SQL code for executing the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c746015-5d4c-4d92-b01e-efd397321005",
   "metadata": {},
   "outputs": [],
   "source": [
    "val ds1 = spark.range(1, 100000000)\n",
    "val ds2 = spark.range(1, 100000000, 2)\n",
    "val ds3 = ds1.repartition(7)\n",
    "val ds4 = ds2.repartition(9)\n",
    "val ds5 = ds3.selectExpr(\"id * 3 as id\")\n",
    "val joined = ds5.join(ds4, \"id\")\n",
    "val filtered = joined.filter(\"id%2 == 1\")\n",
    "val sum = filtered.selectExpr(\"sum(id)\")\n",
    "sum.explain(true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02bfea4f-0110-47c4-beab-a444b6e9da6f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.12.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
