{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6956efa9-7ad7-4446-94c1-e2c207fcf290",
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "\n",
    "val spark = SparkSession.builder\n",
    "    .appName(\"sesion_1\")\n",
    "    .master(\"local[*]\")\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950670f6-dcc4-4437-9bca-c1bb365e606a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%HTML <style>pre { white-space: pre !important; }</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc64f041-2e22-4457-8be7-80c31c3cf0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext.uiWebUrl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68dd8b15-900f-49ee-b046-dff518037c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.{functions => f}\n",
    "import org.apache.spark.sql.{types => t}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b206961-7a2a-433f-895e-26834d4cf524",
   "metadata": {},
   "outputs": [],
   "source": [
    "val contractsDf = spark.read.parquet(\"../../resources/data/parquet/big_contracts\")\n",
    "val clientsDf = spark.read.parquet(\"../../resources/data/parquet/big_clients\")\n",
    "\n",
    "contractsDf.show(2)\n",
    "clientsDf.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e70cc7-0eb9-47f4-a839-72c98e40f1f1",
   "metadata": {},
   "source": [
    "#### spark.sql.adaptive.enabled\n",
    "##### Default value: true since Apache Spark 3.2.0.\n",
    "Adaptive Query Execution (AQE) is an optimization technique in Spark SQL that makes use of the runtime statistics to choose the most efficient query execution plan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3824aecf-ced8-4b49-b200-0eddb1d01c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"false\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806cecb8-9aa7-4771-9e00-8258a3ca2734",
   "metadata": {},
   "outputs": [],
   "source": [
    "contractsDf\n",
    "    .groupBy(\"cod_titular\")\n",
    "    .agg(\n",
    "        f.max(\"fec_alta\"),\n",
    "        f.min(\"fec_alta\")\n",
    "        )\n",
    "    .count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0280c95f-a42b-450a-a8e2-13c73c95fd16",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5518eaf4-d8e2-4b92-b811-06dac0eebe8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "contractsDf\n",
    "    .groupBy(\"cod_titular\")\n",
    "    .agg(\n",
    "        f.max(\"fec_alta\"),\n",
    "        f.min(\"fec_alta\")\n",
    "        )\n",
    "    .count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91fa51d-17f3-45d6-b8b4-8fb1bdbc2e7b",
   "metadata": {},
   "source": [
    "#### spark.sql.autoBroadcastJoinThreshold\n",
    "##### Default value: 10485760 (10 MB)\n",
    "Configures the maximum size in bytes for a table that will be broadcast to all worker nodes when performing a join. By setting this value to -1, broadcasting can be disabled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181de7e0-4639-4e64-9b50-fd1cf71a31b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"false\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dab0f37-e236-40b0-8ea8-238266db2e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df267a9-73b0-4df9-8492-0b091d2afb46",
   "metadata": {},
   "outputs": [],
   "source": [
    "contractsDf.join(clientsDf, Seq(\"id\")).write.mode(\"overwrite\").parquet(\"../../resources/data/parquet/t_broadcast_join\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4295e5-6cd7-4653-86b9-51edca5ce526",
   "metadata": {},
   "outputs": [],
   "source": [
    "10*1024*1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d0e45d-1260-4c77-82f9-ab9761879cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "3*1024*1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e001c52-adeb-4b10-982a-6bb591450214",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", \"3145728\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2859c73-e92c-457b-8a75-6ad2a737aee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "contractsDf.join(clientsDf, Seq(\"id\")).write.mode(\"overwrite\").parquet(\"../../resources/data/parquet/t_broadcast_join\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946a4938-6de2-4884-a1bd-fe609e93395b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.storage.StorageLevel\n",
    "clientsDf.persist(StorageLevel.MEMORY_AND_DISK_SER).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520be839-a618-4367-9c79-928851f7ec4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "clientsDf.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f5d447-cc7d-4ffa-8141-dbdc092076d8",
   "metadata": {},
   "source": [
    "#### spark.sql.adaptive.autoBroadcastJoinThreshold\n",
    "##### Default value: Same as spark.sql.autoBroadcastJoinThreshold\n",
    "Configures the maximum size in bytes for a table that will be broadcast to all worker nodes when performing a join. By setting this value to -1, broadcasting can be disabled. The default value is the same as spark.sql.autoBroadcastJoinThreshold. Note that, this config is used only in adaptive framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea8d812-2ebe-42cd-b5b8-7c4dfc6582a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee2137d-239e-4329-83c9-b597363c43bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.get(\"spark.sql.adaptive.autoBroadcastJoinThreshold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfee1ca1-c9d7-4a7c-9ac4-f3bf571eaec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "contractsDf.join(clientsDf, Seq(\"id\")).write.mode(\"overwrite\").parquet(\"../../resources/data/parquet/t_broadcast_join\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49907aa-c141-4a13-9238-d4474e968b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "2*1024*1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1f9266-42b0-42ab-920a-d38149760fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.adaptive.autoBroadcastJoinThreshold\", \"2097152\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6dc91e4-f7ce-40ed-bd45-bd97055c4a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "contractsDf.join(clientsDf, Seq(\"id\")).write.mode(\"overwrite\").parquet(\"../../resources/data/parquet/t_broadcast_join\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9b3b5e-5d99-4b57-b2d8-f19df3e9aaab",
   "metadata": {},
   "source": [
    "#### spark.sql.broadcastTimeout\n",
    "##### Default value: 300\n",
    "Timeout in seconds for the broadcast wait time in broadcast joins\n",
    "\n",
    "      \"java.util.concurrent.TimeoutException: Futures timed out after [300 seconds] at\"\n",
    "\n",
    "This error is fixed by increasing the value of spark.sql.broadcastTimeout, for example e spark.conf.set(\"spark.sql.broadcastTimeout\", \"3600\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4ded77-b480-4f97-8690-99c20f724eb3",
   "metadata": {},
   "source": [
    "#### spark.default.parallelism\n",
    "##### Default number of partitions in **RDDs** returned by transformations like join, reduceByKey, and parallelize when not set by user.\n",
    "For distributed shuffle operations like reduceByKey and join, the largest number of partitions in a parent RDD. For operations like parallelize with no parent RDDs, it depends on the cluster manager:\n",
    "- Local mode: number of cores on the local machine\n",
    "- Mesos fine grained mode: 8\n",
    "- Others: total number of cores on all executor nodes or 2, whichever is larger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3e215c-5f3f-4862-8315-41663bf549bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "val data = spark.sparkContext.textFile(\"../../resources/data/txt/green_eggs_and_ham.txt\")\n",
    "data.getNumPartitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4212f10a-64a0-4b1d-8274-b5ffe8045664",
   "metadata": {},
   "outputs": [],
   "source": [
    "val result = data\n",
    "    .flatMap(line => line.replace(\"  \", \" \").toUpperCase.split(\" \"))\n",
    "    .map(word => (word, 1))\n",
    "    .reduceByKey((count_1: Int, count_2: Int) => count_1 + count_2)\n",
    "result.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd6a1a2-3f08-4d45-a91a-09d1dd678c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.getNumPartitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53321689-c93d-42af-8f8c-2cf1c055918a",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.default.parallelism\", 17) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ec300d-99ff-4233-a6ea-0fe09fb9f52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "val result = data\n",
    "    .flatMap(line => line.replace(\"  \", \" \").toUpperCase.split(\" \"))\n",
    "    .map(word => (word, 1))\n",
    "    .reduceByKey((count_1: Int, count_2: Int) => count_1 + count_2)\n",
    "result.count()   //No funciona spark.default.parallelism una vez creada la spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd0a8a0-9a09-4d67-a857-6a206076c06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "val data = spark.sparkContext.textFile(\"../../resources/data/txt/green_eggs_and_ham.txt\", 8)\n",
    "data.getNumPartitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a68572-bdac-4076-ba3e-cf047373fcd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "val spark = SparkSession.builder\n",
    "    .config(\"spark.default.parallelism\", \"17\")\n",
    "    .appName(\"sesion_1\")\n",
    "    .master(\"local[*]\")\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b811f5dc-e755-4677-b5c4-f33f006fa63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "val result = data\n",
    "    .flatMap(line => line.replace(\"  \", \" \").toUpperCase.split(\" \"))\n",
    "    .map(word => (word, 1))\n",
    "    .reduceByKey((count_1: Int, count_2: Int) => count_1 + count_2)\n",
    "result.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41baf37-dd3a-494d-ae9a-df70b22ee01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.getNumPartitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7abb67-959d-45ee-8976-297386934a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "val data = spark.sparkContext.parallelize(1 to 100)\n",
    "data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e8c3a4-0530-4451-ab44-c4e1e2313f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.getNumPartitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bddf9c8-5c8f-4799-ae7e-61a85a48d4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.get(\"spark.default.parallelism\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b02b3e-52a5-4731-b24d-9944a00f74b6",
   "metadata": {},
   "source": [
    "#### spark.sql.shuffle.partitions\n",
    "##### Default value: 200\n",
    "The default number of partitions to use when shuffling data for joins or aggregations. Used in DataFrames or DataSets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113b9bd4-e3f5-4eff-8f3b-0991f34feb27",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"false\")\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdcfc269-fdc7-41de-ae8d-cc933c48e19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "contracts_df.join(clients_df, [\"id\"]).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577b02bb-031b-4e58-81d5-51f98b636dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\",\"100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea33b5d-8669-4ea5-baae-6d2434cf218f",
   "metadata": {},
   "outputs": [],
   "source": [
    "contracts_df.join(clients_df, [\"id\"]).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73325f02-f400-48d6-bd0d-21d69c35a5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.get(\"spark.sql.shuffle.partitions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09fff348-6c39-4305-892b-4033cf3cea6f",
   "metadata": {},
   "source": [
    "#### spark.sql.files.maxPartitionBytes\t\n",
    "##### Default value: 134217728 (128 MB)\n",
    "The maximum number of bytes to pack into a single partition when reading files. This configuration is effective only when using file-based sources such as Parquet, JSON and ORC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2c97c1-0f88-40d9-a2b7-20f2cf364ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "contractsDf.join(clientsDf, Seq(\"id\")).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ef749c-4257-4c7f-8a96-e9c0d16b063a",
   "metadata": {},
   "source": [
    "#### spark.sql.adaptive.skewJoin.enabled\n",
    "##### Default value: true\n",
    "When true and spark.sql.adaptive.enabled is true, Spark dynamically handles skew in sort-merge join by splitting (and replicating if needed) skewed partitions.\n",
    "\n",
    "https://spark.apache.org/docs/latest/sql-performance-tuning.html#optimizing-skew-join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018631fe-1ad9-4ff4-b71a-6621dff27050",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\")\n",
    "spark.conf.set(\"spark.sql.adaptive.autoBroadcastJoinThreshold\", \"-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30a6859-ebbe-4d51-b5cb-42f4f9bf1445",
   "metadata": {},
   "outputs": [],
   "source": [
    "contractsDf.groupBy(\"cod_titular\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0968d27d-1913-49ce-b854-815b76959f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "clientsDf.sample(false, 0.00003, 0).groupBy(\"cod_client\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e77503-0063-47c4-8d43-81236383cce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "contractsDf.join(clientsDf.sample(false, 0.00003, 0).drop(\"id\"), f.col(\"cod_titular\") === f.col(\"cod_client\"))\n",
    "    .write.mode(\"overwrite\").parquet(\"../../resources/data/parquet/t_skew_join\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac0df23-9fc7-4aed-8939-b76af1ca5c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.adaptive.skewJoin.enabled\",\"true\")\n",
    "spark.conf.set(\"spark.sql.adaptive.skewJoin.skewedPartitionFactor\", \"2.0\")\n",
    "spark.conf.set(\"spark.sql.adaptive.skewJoin.skewedPartitionThresholdInBytes\", \"50m\")\n",
    "spark.conf.set(\"spark.sql.adaptive.advisoryPartitionSizeInBytes\", \"10m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e239f0-526b-4fa1-836b-207e0c51c25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "contractsDf.join(clientsDf.sample(false, 0.00003, 0).drop(\"id\"), f.col(\"cod_titular\") === f.col(\"cod_client\"))\n",
    "    .write.mode(\"overwrite\").parquet(\"../../resources/data/parquet/t_skew_join\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9927a0d1-a20e-45a7-bb24-686a8072629c",
   "metadata": {},
   "source": [
    "#### spark.sql.adaptive.coalescePartitions.enabled\n",
    "##### Default value: true\n",
    "When true and spark.sql.adaptive.enabled is true, Spark will coalesce contiguous shuffle partitions according to the target size (specified by spark.sql.adaptive.advisoryPartitionSizeInBytes), to avoid too many small tasks.\n",
    "\n",
    "https://spark.apache.org/docs/latest/sql-performance-tuning.html#coalescing-post-shuffle-partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1722fa4b-6046-4fd3-a952-ded322e6b308",
   "metadata": {},
   "outputs": [],
   "source": [
    "val contractsDf = spark.read.parquet(\"../../resources/data/parquet/big_contracts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48df734f-b205-453a-86d5-1b66934c73bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"false\")\n",
    "\n",
    "contractsDf.repartition(f.col(\"id\"))\n",
    "    .write.mode(\"overwrite\").parquet(\"../../resources/data/parquet/t_skew_join\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebaa377b-d74e-44a9-a26c-92c8f22b39cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "\n",
    "contractsDf.repartition(f.col(\"id\"))\n",
    "    .write.mode(\"overwrite\").parquet(\"../../resources/data/parquet/t_skew_join\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f88f1d4-b305-4505-9547-ec3f129c7672",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.adaptive.coalescePartitions.parallelismFirst\", \"false\")\n",
    "spark.conf.set(\"spark.sql.adaptive.advisoryPartitionSizeInBytes\", \"64m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36b977d-8e8d-49e5-968f-7c6041e0e54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "contractsDf.repartition(f.col(\"id\"))\n",
    "    .write.mode(\"overwrite\").parquet(\"../../resources/data/parquet/t_skew_join\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea621c79-dd00-4b0a-abf9-72652e7f5e38",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.12.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
